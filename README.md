# ğŸ§  Sign-Language-Detection-Model

This project is a Real-Time Sign Language Detection System that recognizes English alphabet signs (Aâ€“Z) using a webcam. It leverages MediaPipe for hand landmark detection and a Machine Learning model trained with custom-collected gesture data.

ğŸš€ Features

ğŸ“¸ Real-time hand gesture detection using OpenCV
âœ‹ Hand landmark tracking with MediaPipe Hands
ğŸ§© Custom-trained ML model for Aâ€“Z alphabet recognition
ğŸ§  Efficient and lightweight â€” runs smoothly on most systems
ğŸ’¾ Includes data collection, training, and prediction scripts

ğŸ§° Tech Stack

Python
OpenCV
MediaPipe
NumPy
Scikit-learn
Pickle

ğŸ“‚ Project Structure
â”‚
â”œâ”€â”€ collect_images.py      # Capture and save hand gesture data
â”œâ”€â”€ create_dataset.py      # Making the folders of the collected data
â”œâ”€â”€ train_classifier.py    # Training the Random Forest Regressor model 
â”œâ”€â”€ test_model.p           # Run real-time sign recognition
â””â”€â”€ requirements           # The versions of the tech stack required

ğŸ§‘â€ğŸ’» How It Works

Data Collection â€“ Capture images for each alphabet using collect_images.py.
Model Training â€“ Train a classifier using extracted landmarks.
Real-Time Prediction â€“ Detect hand landmarks live and predict corresponding alphabet signs.

ğŸ¯ Future Enhancements

Support for dynamic gestures (e.g., words or phrases) (Requiring huge dataset with more advanced GPUs)
Integration with Deep Learning (CNN) models
Add audio feedback for recognized signs
